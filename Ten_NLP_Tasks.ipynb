{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kumartr/NLP/blob/master/Ten_NLP_Tasks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "kG0DIiFnrwts",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fvBtmAajrzCY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "1. Stemming"
      ]
    },
    {
      "metadata": {
        "id": "dbtNOUpGr0f2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "4f9e963c-1afe-4a2a-b7d3-c74b9076f64c"
      },
      "cell_type": "code",
      "source": [
        "!pip install stemming\n",
        "from stemming.porter2 import stem\n",
        "stem(\"casually\")\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting stemming\n",
            "  Downloading https://files.pythonhosted.org/packages/d1/eb/fd53fb51b83a4e3b8e98cfec2fa9e4b99401fce5177ec346e4a5c61df71e/stemming-1.0.1.tar.gz\n",
            "Building wheels for collected packages: stemming\n",
            "  Running setup.py bdist_wheel for stemming ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/e8/05/2e/2ddeb64d4464b854b48323f9676528c17560da7d153db7b0e2\n",
            "Successfully built stemming\n",
            "Installing collected packages: stemming\n",
            "Successfully installed stemming-1.0.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'casual'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "lYaBGJGasupF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "2. Lemmatizer"
      ]
    },
    {
      "metadata": {
        "id": "3alnbbZ9swkF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        },
        "outputId": "31590880-d027-4146-b2a7-e48d30219166"
      },
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en\n",
        "import spacy\n",
        "lem_spacy=spacy.load(\"en\")\n",
        "doc=\"good better best\"\n",
        "\n",
        "for token in lem_spacy(doc):\n",
        "    print(token,token.lemma_)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.0.18)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.15.4)\n",
            "Requirement already satisfied: thinc<6.13.0,>=6.12.1 in /usr/local/lib/python3.6/dist-packages (from spacy) (6.12.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: dill<0.3,>=0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.8.2)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.9.6)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.18.4)\n",
            "Requirement already satisfied: regex==2018.01.10 in /usr/local/lib/python3.6/dist-packages (from spacy) (2018.1.10)\n",
            "Requirement already satisfied: ujson>=1.35 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.35)\n",
            "Requirement already satisfied: msgpack<0.6.0,>=0.5.6 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy) (0.5.6)\n",
            "Requirement already satisfied: wrapt<1.11.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy) (1.10.11)\n",
            "Requirement already satisfied: cytoolz<0.10,>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy) (0.9.0.1)\n",
            "Requirement already satisfied: msgpack-numpy<0.4.4 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy) (0.4.3.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy) (4.28.1)\n",
            "Requirement already satisfied: six<2.0.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy) (1.11.0)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.22)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.6)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2018.11.29)\n",
            "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cytoolz<0.10,>=0.9.0->thinc<6.13.0,>=6.12.1->spacy) (0.9.0)\n",
            "Requirement already satisfied: en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0 in /usr/local/lib/python3.6/dist-packages (2.0.0)\n",
            "\n",
            "\u001b[93m    Linking successful\u001b[0m\n",
            "    /usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "    /usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "\n",
            "    You can now load the model via spacy.load('en')\n",
            "\n",
            "good good\n",
            "better better\n",
            "best good\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mdBXGx-0wSfC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "3. Word Embeddings"
      ]
    },
    {
      "metadata": {
        "id": "HJ_5OX0PtG1w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install gensim\n",
        "#git clone 'https://github.com/mmihaltz/word2vec-GoogleNews-vectors'\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "#word_vectors=KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz',binary=True)\n",
        "#word_vectors['human']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BwaRSede3URa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "https://github.com/kavgan/nlp-text-mining-working-examples/blob/master/word2vec/Word2Vec.ipynb\n",
        "http://kavita-ganesan.com/kavitas-tutorials/\n"
      ]
    },
    {
      "metadata": {
        "id": "kJr0ST3bygI4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "outputId": "848cd901-2fad-4636-b5b6-8c3a32e5893c"
      },
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "!python -m spacy download en_core_web_md\n",
        "spacy_similar=spacy.load(\"en_core_web_md\")  # make sure to use larger model!\n",
        "tokens = spacy_similar(u'dog cat banana')\n",
        "\n",
        "for token1 in tokens:\n",
        "    for token2 in tokens:\n",
        "        print(token1.text, token2.text, token1.similarity(token2))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting en_core_web_md==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.0.0/en_core_web_md-2.0.0.tar.gz#egg=en_core_web_md==2.0.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.0.0/en_core_web_md-2.0.0.tar.gz (120.8MB)\n",
            "\u001b[K    100% |████████████████████████████████| 120.9MB 46.3MB/s \n",
            "\u001b[?25hInstalling collected packages: en-core-web-md\n",
            "  Running setup.py install for en-core-web-md ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \bdone\n",
            "\u001b[?25hSuccessfully installed en-core-web-md-2.0.0\n",
            "\n",
            "\u001b[93m    Linking successful\u001b[0m\n",
            "    /usr/local/lib/python3.6/dist-packages/en_core_web_md -->\n",
            "    /usr/local/lib/python3.6/dist-packages/spacy/data/en_core_web_md\n",
            "\n",
            "    You can now load the model via spacy.load('en_core_web_md')\n",
            "\n",
            "dog dog 1.0\n",
            "dog cat 0.80168545\n",
            "dog banana 0.24327643\n",
            "cat dog 0.80168545\n",
            "cat cat 1.0\n",
            "cat banana 0.28154364\n",
            "banana dog 0.24327643\n",
            "banana cat 0.28154364\n",
            "banana banana 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RF3Hd9ma4acX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "5. Part of Speech Tagging"
      ]
    },
    {
      "metadata": {
        "id": "oozFidrHwRnC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "fc4b07ef-b4bd-46f9-81b9-bcab75537d53"
      },
      "cell_type": "code",
      "source": [
        "#!pip install spacy\n",
        "!python -m spacy download en \n",
        "nlp=spacy.load('en')\n",
        "sentence=\"Ashok killed the snake with a stick\"\n",
        "for token in nlp(sentence):\n",
        "   print(token,token.pos_)\n",
        "\n",
        " "
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz (37.4MB)\n",
            "\u001b[K    100% |████████████████████████████████| 37.4MB 49.9MB/s \n",
            "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
            "  Running setup.py install for en-core-web-sm ... \u001b[?25l-\b \b\\\b \b|\b \bdone\n",
            "\u001b[?25hSuccessfully installed en-core-web-sm-2.0.0\n",
            "\n",
            "\u001b[93m    Linking successful\u001b[0m\n",
            "    /usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "    /usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "\n",
            "    You can now load the model via spacy.load('en')\n",
            "\n",
            "Ashok PROPN\n",
            "killed VERB\n",
            "the DET\n",
            "snake NOUN\n",
            "with ADP\n",
            "a DET\n",
            "stick NOUN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cYhB2ttZ556P",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "6. Named Entity Types"
      ]
    },
    {
      "metadata": {
        "id": "uhmSKQVc59q_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "7897ae03-c98a-4dac-a684-0dd3999f884f"
      },
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp=spacy.load('en')\n",
        "sentence=\"Ram of Apple Inc. travelled to Sydney on 5th October 2017\"\n",
        "sentence=\"Apple earned a revenue of 200 Billion USD in 2016\"\n",
        "for token in nlp(sentence):\n",
        "   print(token, token.ent_type_)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Apple ORG\n",
            "earned \n",
            "a \n",
            "revenue \n",
            "of \n",
            "200 MONEY\n",
            "Billion MONEY\n",
            "USD MONEY\n",
            "in \n",
            "2016 DATE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZSvnZwjI97Wk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "10. Text Summarization"
      ]
    },
    {
      "metadata": {
        "id": "YBOg36kT99nX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "outputId": "a03c3841-41dc-4810-8fb9-589a786414d7"
      },
      "cell_type": "code",
      "source": [
        "from gensim.summarization import summarize\n",
        "\n",
        "sentence=\"Automatic summarization is the process of shortening a text document with software, in order to create a summary with the major points of the original document. Technologies that can make a coherent summary take into account variables such as length, writing style and syntax.Automatic data summarization is part of machine learning and data mining. The main idea of summarization is to find a subset of data which contains the information of the entire set. Such techniques are widely used in industry today. Search engines are an example; others include summarization of documents, image collections and videos. Document summarization tries to create a representative summary or abstract of the entire document, by finding the most informative sentences, while in image summarization the system finds the most representative and important (i.e. salient) images. For surveillance videos, one might want to extract the important events from the uneventful context.There are two general approaches to automatic summarization: extraction and abstraction. Extractive methods work by selecting a subset of existing words, phrases, or sentences in the original text to form the summary. In contrast, abstractive methods build an internal semantic representation and then use natural language generation techniques to create a summary that is closer to what a human might express. Such a summary might include verbal innovations. Research to date has focused primarily on extractive methods, which are appropriate for image collection summarization and video summarization.\"\n",
        "\n",
        "summarize(sentence)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Document summarization tries to create a representative summary or abstract of the entire document, by finding the most informative sentences, while in image summarization the system finds the most representative and important (i.e. salient) images.\\nExtractive methods work by selecting a subset of existing words, phrases, or sentences in the original text to form the summary.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    }
  ]
}